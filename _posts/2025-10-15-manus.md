---
layout: post
comments: false
title:  "Context Engineering in Manus"
excerpt: "Manus approaches to context engineering."
date:   2025-10-15 
---

[Lance Martin](https://x.com/RLanceMartin)

## Why Context Engineering

> Earlier this week, I had a webinar with [Manus co-founder and CSO Yichao "Peak" Ji](https://luma.com/819i5ime). You can see the video [here](https://youtu.be/6_BcCthVvb8?si=o8ovK6YNWOXtq7j7), my slides [here](https://drive.google.com/file/d/1QGJ-BrdiTGslS71sYH4OJoidsry3Ps9g/view), and Peak's slides [here](https://docs.google.com/presentation/d/1Z-TFQpSpqtRqWcY-rBpf7D3vmI0rnMhbhbfv01duUrk/edit?usp=sharing). Below are my notes.
 
[Anthropic defines](https://www.anthropic.com/engineering/building-effective-agents) agents as systems where LLMs direct their own processes and tool usage, maintaining control over how they accomplish tasks. In short, it's an LLM calling tools in a loop.

[Manus](https://en.wikipedia.org/wiki/Manus_(AI_agent)) is one of the most popular [general-purpose consumer agents](https://x.com/manusai_hq?lang=en). The typical Manus task uses [50 tool calls](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus). Without context engineering, these tool call results would accumulate in the LLM context window. As the context window fills, many have observed that LLM performance degrades. 

For example, Chroma has a great study on [context rot](https://research.trychroma.com/context-rot) and Anthropic has [explained](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents) how growing context depletes an LLM's attention budget. So, it's important to carefully manage what goes into the LLM's context window when building agents. [Karpathy laid this out clearly](https://x.com/karpathy/status/1937902205765607626?ref=blog.langchain.com):    

> Context engineering is the delicate art and science of filling the context window with just the right information for the next step (in an agent's trajectory)

## Context Engineering Approaches

Each Manus session runs in full virtual machine, giving the agent access to a filesystem and a terminal with various utilities. Manus uses three primary strategies for context engineering, which align with approaches Anthropic covers [here](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents) and [I've seen in across many projects](https://docs.google.com/presentation/d/1Z-TFQpSpqtRqWcY-rBpf7D3vmI0rnMhbhbfv01duUrk/edit?usp=sharing):

* **Reduce**
* **Offload** 
* **Isolate**

## Reduce: Compaction and Summarization

Many tool calls in Manus have a "full" and "compact" representation. The full version contains the raw content from tool invocation (e.g., a complete search tool result), which is stored in the sandbox (e.g., filesystem). The compact version stores a reference to the full result (e.g., a file path).

<figure>
<img src="/assets/manus_compaction.png" width="90%">
<figcaption>
</figcaption>
</figure>

Manus uses a policy to apply compaction as the agent approaches the context window limit (e.g., older tool results will be replaced with the compact version). Newer tool results remain in full to guide the agent's next decision. This approach is similar to Anthropic's recent [context editing](https://www.anthropic.com/news/context-management) feature.

When compaction reaches diminishing returns (see figure below), Manus applies summarization. Summaries are generated from the full tool results and Manus uses a schema to define the fields for summarization. This creates a consistent summary object for any agent trajectory. 

<figure>
<img src="/assets/manus_reduction.png" width="90%">
<figcaption>
</figcaption>
</figure>

## Isolate: Strategic Sub-Agent Design

Manus takes a pragmatic approach to sub-agents, avoiding the anthropomorphized divisions of labor common in some multi-agent systems. While humans organize by role (designer, engineer, project manager) due to cognitive limitations, Manus recognizes that LLMs don't share these same constraints.

The system uses just a few specialized sub-agents: a planner that manages work, a knowledge manager that reviews conversations and determines what should be saved in the filesystem, and an executor for delegating tasks. This minimalism stems from the fact that [inter-agent communication is difficult](https://cognition.ai/blog/dont-build-multi-agents) and every sub-agent adds communication overhead.

Manus initially used a `todo.md` for task planning, but found that roughly one-third of all actions were spent updating the todo list, wasting valuable tokens. They shifted to a sub-agent approach. A dedicate planner agent just calls a sub-agents (executor) as a tool calls for tasks.

Syncing context between the planner and the executor presents a challenge. For simple tasks (e.g., the planner only needs the output), the parent sends instructions to the sub-agent. This pattern resembles [Claude Code's task tool](https://claudelog.com/faqs/what-is-task-tool-in-claude-code/). 

<figure>
<img src="/assets/manus_isolation.png" width="90%">
<figcaption>
</figcaption>
</figure>

For more complex scenarios (e.g., the sub-agent needs to know the full context or write to  files that the parents also uses), the planner can share its *full* context with the executor sub-agent. The sub-agent still has its own action space (tools) and prompt, but receives the full context of the parent (e.g., the full conversation history). 

In all cases, the planner defines the sub-agent's output schema. Sub-agents have a "submit results" tool to populate this schema before returning results to the planner. Manus uses constrained decoding to ensure outputs adhere to the defined schema. 

## Offload: A Layered Action Space

When building agents, it is tempting to use a large collection of tools. But, tool descriptions utilize value tokens (e.g., in the agent's system prompt) and an excessive number of (often overlapping or ambiguous) tools [can cause model confusion](https://www.anthropic.com/news/context-management).

One approach is to [index tool descriptions](https://github.com/langchain-ai/langgraph-bigtool) and retrieve tools on-the-fly based each task. However, this [can confuse the model](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus) if retrieved tools differ from tools called at prior agent turns.

Manus spent a lot of time designing a layered action space with function calling and a sandbox. The function calling layer is *minimal*, containing fewer than <20 atomic functions that the LLM can call. With simple but general functions - such as a shell tool (that executes arbitrary shell commands) and a text editor (that read and write files) - an agent can perform an *extremely wide range of actions*. 

<figure>
<img src="/assets/manus_offloading.png" width="90%">
<figcaption>
</figcaption>
</figure>

These actions are executed in the sandbox, similar to the [CodeACT](https://arxiv.org/abs/2402.01030) architecture. The key point is this: Manus pushes many actions to the sandbox layer, rather than the function calling layer. Specifically, it includes many utilities (e.g., speech tools, MCP CLI, etc) as shell commands. The agent's instructions briefly mention that utilities are available and instructs the agent to use commands (e.g., `--help` flags) to learn about them.

This design reduces the number of functions that the LLM can call, reducing cognitive load and saving tokens in the agent instructions. Peak specifically mentioned that MCP tools are pushed to the sandbox layer and can be called via a CLI via the agent's shell tool.

In addition, Manus has access to a filesystem. Like Claude Code, Manus uses basic UNIX utilities (e.g., `glob` and `grep`) to search the filesystem. It does not rely on indexing or vectorstores, but Peak mentioned that indexing is useful in the context of (larger) document collections (e.g., enterprise knowledge bases, etc) and can be connected to Manus via MCP. 

A task that requires stock analysis analysis might look like this: the planner agent will spawn an executor sub-agent with instructions. The executor sub-agent makes a function call to write a script that implements some analysis logic (e.g., it may call search or stock APIs in the sandbox). The sub-agent then makes a function call to execute the script and writes the results to a log. It might also make a function call that runs a specific utility (e.g., via the MCP CLI) that further analyzes the log. Finally, it makes a function call to read the results from the log and returns a summary of the results to the planner agent following the schema defined by the planner.

## Model Choice

Manus's approach to model selection is driven by cost considerations, [with KV cache efficiency playing a central role](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus). They use caching aggressively (e.g., for system instructions and other repeated content) to reduce both cost and latency. Distributed KV cache infrastructure is challenging to implement with open source models, but is [well-supported by frontier providers](https://www.anthropic.com/news/prompt-caching). This support for caching actually makes frontier models cheaper (than open source models) in practice. Rather than committing to a single model, Manus mixes and matches with task-level routing: for example, they might use Claude for coding, Gemini for multi-modal tasks, and OpenAI for math and reasoning. This pragmatism extends to their stance on reinforcement learning and fine-tuning.

## Build with the Bitter Lesson in Mind

We closed the discussion talking [about the Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html). I've been interested [in its implications for AI engineering](https://rlancemartin.github.io/2025/07/30/bitter_lesson/). Boris Cherny of Claude Code mentioned that [The Bitter Lesson](https://www.youtube.com/watch?v=Lue8K2jqfKk) influenced his approach. Keeping Claude Code simple and unopinionated makes it easier to adapt to model improvements.

Building on constantly improving models means accepting constant change. Peak mentioned that Manus has been refactored five times since their launch in March! In addition, Peak warned that the agent's harness can limit performance as models advance; this is exactly the challenge called out by the Bitter Lesson. We add structure to improve performance at a point in time, but this structure can limit performance as compute (models) grows.

To guard against this, Peak suggested running agent evaluations across varying model strengths. If performance doesn't improve with stronger models, your harness may be hobbling the agent. This can help test whether your harness is "future proof".

Hyung Won Chung's (OpenAI/MSL) [talk](https://youtu.be/orDKvo8h71o?si=fsZesZuP25BU6SqZ) on this topic further emphasizes the need to consistently re-evaluate structure (e.g., your harness) as models improve.

> *Add structures needed for the given level of compute and data available. 
Remove them later, because these shortcuts will bottleneck further improvement.*
> 

<figure>
<img src="/assets/bitter_lesson_timeline.png" width="90%">
<figcaption>
</figcaption>
</figure>

## Conclusions

Some take-away lessons from the discussion:

**Context Engineering**
1. **Reduce** - Compact older tool results (e.g., with a link to a file) and apply summarization once compaction reaches diminishing returns
2. **Offload** - Save tool results external to the context window (e.g., in the filesystem) so that they can be accessed on demand
3. **Isolate** - Delegate tasks to sub-agents with their own context windows, sharing either instructions or the full context depending on task complexity

**Action Space Design:**
4. **Fewer general tools** - Use minimal function calls (<20) that access general utilities (shell, editor), giving the model a wide range of capabilities without bloating the function calling layer
5. **Enable discovery** - Let agents discover utilities (e.g., via `--help` flags) through a single shell tool rather than bloating instructions 
6. **Avoid tool retrieval** - Dynamic tool retrieval can confuse models; prefer a stable, layered architecture (e.g., function calling that can access many sandbox utilities)

**Architectural Decisions:**
7. **Sub-agents are tools, not teams** - Don't anthropomorphize agent roles; use sub-agents for context isolation (to perform tasks), not cognitive division of labor (e.g., engineer, project manager)
8. **Share context deliberately** - Pass minimal instructions for simple tasks, full context for complex ones; always define output schemas for sub-agent results
9. **Cache aggressively** - KV caching improves cost and latency; distributed KV cache infrastructure is challenging to implement with open source models, but is [supported by frontier providers](https://www.anthropic.com/news/prompt-caching)

**Development Philosophy:**
10. **Design for model evolution** - If stronger models don't improve your agent's performance, your harness may be limiting them (test across model strengths)
11. **Stay unopinionated** - Simple, flexible architectures adapt better to rapid model improvements than heavily structured ones
12. **Embrace the Bitter Lesson** - Don't be afraid to change your architecture as models improve, and constantly re-evaluate your assumptions