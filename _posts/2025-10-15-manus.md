---
layout: post
comments: false
title:  "Context Engineering in Manus"
excerpt: "Manus approaches to context engineering."
date:   2025-10-15 
---

[Lance Martin](https://x.com/RLanceMartin)

## Why Context Engineering

> Preface: Earlier this week, I had a webinar with [Manus co-founder and CSO Yichao "Peak" Ji](https://luma.com/819i5ime). You can see the video [here](https://youtu.be/6_BcCthVvb8?si=o8ovK6YNWOXtq7j7), my slides [here](https://drive.google.com/file/d/1QGJ-BrdiTGslS71sYH4OJoidsry3Ps9g/view), and Peak's slides [here](https://docs.google.com/presentation/d/1Z-TFQpSpqtRqWcY-rBpf7D3vmI0rnMhbhbfv01duUrk/edit?usp=sharing). Below are my notes from Peak's talk and our Q&A.
 
[Anthropic defines](https://www.anthropic.com/engineering/building-effective-agents) agents as systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks. In short, agents are an LLM calling tools autonomously in a loop.

[Manus](https://en.wikipedia.org/wiki/Manus_(AI_agent)) is one of the most popular [general-purpose consumer agent](https://x.com/manusai_hq?lang=en). The typical Manus task uses [50 tool calls](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus). Without context engineering, these tool call results would accumulate in the LLM context window. As the context window fills, many have observed that LLM performance degrades. 

For example, Chroma has a great study on [context rot](https://research.trychroma.com/context-rot) and Anthropic has [explained](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents) how growing context depletes an LLM's attention budget. So, it's important to carefully manage what goes into the LLM's context window when building agents. [Karpathy stated this clearly](https://x.com/karpathy/status/1937902205765607626?ref=blog.langchain.com):    

> Context engineering is the delicate art and science of filling the context window with just the right information for the next step (in an agent's trajectory)

## Context Engineering Approaches

Each Manus session runs in full virtual machine, giving the agent access to a filesystem and a terminal with various utilities explained below. Within this environment, Manus uses three primary strategies for context engineering. These align with approaches [I've seen in across many other projects](https://docs.google.com/presentation/d/1Z-TFQpSpqtRqWcY-rBpf7D3vmI0rnMhbhbfv01duUrk/edit?usp=sharing) and with the 3 categories mentioned by Anthropic [here](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents):

* **Reduce**: Minimize the size of information added to the context window through smart tool design, compaction of tool outputs, and summarization of conversation history.
* **Offload**: Keep raw data and tool observations external (e.g., in the filesystem or memory of a sandbox environment) to the agent's context window.
* **Isolate**: Delegate specific tasks to specialized sub-agents that operate with their own isolated context windows, preventing context bloat in the main agent.

## Reduce: Compaction and Summarization

Many tool calls in Manus have a "full" and "compact" representation. The full version contains the full content from tool invocation (e.g., a complete search tool result), which is stored in the sandbox (e.g., filesystem). The compact version stores a reference to the full result (e.g., a file path or url).

<figure>
<img src="/assets/manus_compaction.png" width="90%">
<figcaption>
</figcaption>
</figure>

Manus uses a policy to apply compaction as the agent approaches the context window limit based on recency (e.g., the oldest % of tool result messages will be replaced with the compact version). Newer tool calls remain in full to guide its next decision. This approach is similar to Anthropic's recent [context editing](https://www.anthropic.com/news/context-management) feature.

When compaction reaches diminishing returns see figure below, Manus then applies summarization. Critically, summaries are always generated from the full tool results. Rather than using free-form prompting, Manus uses a schema to define specific fields for extraction during summarization. This creates stable, structured summary outputs that are consistent. 

<figure>
<img src="/assets/manus_reduction.png" width="90%">
<figcaption>
</figcaption>
</figure>

## Isolate: Strategic Sub-Agent Design

Manus takes a pragmatic approach to sub-agents, avoiding the anthropomorphized divisions of labor common in some multi-agent systems. While humans organize by role (designer, engineer, project manager) due to cognitive limitations, Manus recognizes that LLMs don't share these same constraints.

The system uses just a few specialized sub-agents: a planner that manages work, a knowledge manager that reviews conversations and determines what should be saved in the filesystem, and an executor for delegating tasks. This minimalism stems from the fact that [inter-agent communication is difficult](https://cognition.ai/blog/dont-build-multi-agents) and every sub-agent adds communication overhead.

Manus initially used a `todo.md` for task planning, but found that roughly one-third of all actions were spent updating the todo list, wasting valuable tokens! They shifted to an agent-as-tool approach where a planner calls a sub-agent (executor) as a tool call for specific tasks.

Syncing context between the planner agent and the executor sub-agent presents challenges that Manus addresses through two approaches. For tasks simple tasks where the planner only needs the output, they share instructions: the parent sends custom instructions to the sub-agent, whose context contains only these instructions and relevant tools. This pattern resembles [Claude Code's task tool](https://claudelog.com/faqs/what-is-task-tool-in-claude-code/). 

<figure>
<img src="/assets/manus_isolation.png" width="90%">
<figcaption>
</figcaption>
</figure>

For more complex scenarios where the sub-agent needs to know the full context or write to a files that the parents also uses, the parent shares its *full* context with the sub-agent. The sub-agent still has its own action space (tools) and prompt, but receives a large pre-filled input containing the full context window of the parent (e.g., the full conversation history). 

Across all cases, the planner defines the sub-agent's output schema. Sub-agents have a "submit results" tool to populate this schema before returning results to the planner. Manus uses constrained decoding to ensure outputs adhere to the defined schema. 

## Offload: A Layered Action Space

When building agents, it is tempting to provide them with a large collection of tools. In practice, tool descriptions utilize valuable tokens in the agent's instructions and an excessive number of (overlapping) tools [can cause model confusion](https://www.anthropic.com/news/context-management).

One approach is to [index tool descriptions](https://github.com/langchain-ai/langgraph-bigtool) and retrieve specific tools on demand based on semantic similarity to a given task. However, this [can confuse the model](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus) if retrieved tools differ from tools called at prior agent turns.

Manus uses a layered action space with three tiers to simplify this: function calling, sandbox utilities, and packages/APIs. The function calling layer is *minimal*, containing fewer than 10-20 atomic functions. With simple but general functions - such as a shell tool (that executes arbitrary shell commands) and a text editor (that read and write files) - an agent can perform an extremely wide range of tasks. 

<figure>
<img src="/assets/manus_offload.png" width="90%">
<figcaption>
</figcaption>
</figure>

These tasks are executed in the sandbox layer, which is similar to the [CodeACT](https://arxiv.org/abs/2402.01030) paper. This approach is elegant, because many pre-installed utilities (e.g., speech tools, MCP CLI, etc) can be exposed as shell commands and executed in the sandbox.  The agent's system prompt only needs to briefly mention that utilities are available; the agent can simply call shell tools with `--help` flags to learn about any one utility.

This design allows Manus to add new capabilities without touching the model's function calling space. MCP tools, for instance, aren't injected into function calls but are added to the Sandbox layer as CLI utilities. The agent discovers them organically with shell commands.

In addition, Manus has access to a filesystem. Like Claude Code, Manus uses basic UNIX utilities like glob and grep to search the filesystem. It does not rely on indexing or vector stores, but Peak mentioned that indexing is useful in the context of (larger) document collections (e.g., memories, enterprise knowledge bases, etc).

As an example, for a task that requires stock analysis analysis the flow would go like this: the planner agent will spawn an executor sub-agent to perform the analysis. The executor sub-agent makes a tool call to write a script that implements the analysis logic (e.g., it ma calls any number of the search or stock information APIs available in the sandbox). 

The sub-agent then makes a tool call to execute the script and write the results to a log. It can then read the results from the log and return a summary of the results to the planner agent following the schema defined by the planner.

## Model Choice: Pragmatism Over Purity

Manus's approach to model selection is driven by cost considerations, [with KV cache efficiency playing a central role](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus). They use caching aggressively (e.g., for system instructions and other repeated content) to reduce both cost and latency. Distributed KV cache infrastructure is challenging to implement with open source models, but is [well-supported by frontier providers](https://www.anthropic.com/news/prompt-caching). This support for caching actually makes frontier models cheaper (than open source models) in practice.

Rather than committing to a single model, Manus mixes and matches with task-level routing: for example, they might use Claude for coding, Gemini for multi-modal tasks, and OpenAI for math and reasoning. This pragmatism extends to their stance on reinforcement learning and fine-tuning.

Manus does not use reinforcement fine-tuning to build specialized models for their task(s). This is because reinforcement learning works well with a fixed action space. Yet, fixing the action space is difficult in the current agent landscape, with things changing rapidly.

The launch of MCP exemplifies this challenge. Supporting MCP transformed Manus from having a static action space to requiring an extensible one that the user can configure, creating a much harder open-domain problem for RL. They found it better to lean on general foundation models and context engineering. 

## Build with the Bitter Lesson in Mind

We closed the discussion talking about the Bitter Lesson. I've been very interested [in this topic and its implications for AI engineering](https://rlancemartin.github.io/2025/07/30/bitter_lesson/). Boris of Claude Code mentions that [The Bitter Lesson](https://www.youtube.com/watch?v=Lue8K2jqfKk) strongly influenced his approach. Keeping Claude Code simple and unopinionated made it easier to adapt to model improvements.

Building on constantly improving models means accepting constant change. Peak mentioned that Manus has refactored five times since their launch in March! In addition, Peak warned that the agent's harness can limit performance as models advance; this is exactly the challenge called out by the Bitter Lesson. We add structure to improve performance at a point in time, but this structure can also limit performance as models advance.

To guard against this, Peak suggested running agent evaluations across varying model strengths. If performance doesn't improve with stronger models, your harness may be hobbling the agent. This provides a simple test of whether your harness is "future proof".

Hyung Won Chung's (OpenAI/MSL) [talk](https://youtu.be/orDKvo8h71o?si=fsZesZuP25BU6SqZ) on this topic further emphasizes the need to consistently re-evaluate structure (e.g., your harness) as models improve.

> *Add structures needed for the given level of compute and data available. 
Remove them later, because these shortcuts will bottleneck further improvement.*
> 

<figure>
<img src="/assets/bitter_lesson_timeline.png" width="90%">
<figcaption>
</figcaption>
</figure>

## Conclusions

**Context Engineering**
1. **Reduce** - Compact older tool results (e.g., with a link to a file) in the agent messages and apply summarization once compaction reaches diminishing returns
2. **Offload** - Save tool results external to the context window (e.g., in the filesystem) so that they can always be accessed on demand
3. **Isolate** - Delegate tasks to executor sub-agents with their own context windows, sharing either instructions or the full context depending on the task complexity

**Action Space Design:**
4. **Small number of general tools** - Use minimal function calls (<20) that access general utilities (shell, editor), giving the model a wide range of capabilities without bloating the function calling layer
5. **Enable organic discovery** - Let agents discover utilities (e.g., via `--help` flags) access through a simple shell tool, rather than bloating instructions or, worse, excessive tools in the function calling layer
6. **Avoid tool retrieval** - Dynamic tool retrieval can confuse models; prefer a stable, layered architecture (e.g., function calling that can access many sandbox utilities)

**Architectural Decisions:**
7. **Sub-agents are tools, not teams** - Don't anthropomorphize agent roles; use sub-agents for context isolation (to perform tasks), not cognitive division of labor (e.g., designer, engineer, project manager)
8. **Share context deliberately** - Pass minimal instructions for simple tasks, full context for complex ones; always define output schemas for sub-agent results
9. **Cache aggressively** - Aggressive KV caching improves both cost and latency; distributed KV cache infrastructure is challenging to implement with open source models, but is [well-supported by frontier providers](https://www.anthropic.com/news/prompt-caching)

**Development Philosophy:**
10. **Design for model evolution** - If stronger models don't improve your agent's performance, your harness may be limiting them (test across model strengths)
11. **Stay unopinionated** - Simple, flexible architectures adapt better to rapid model improvements than heavily structured ones
12. **Embrace the Bitter Lesson** - Don't be afraid to change your architecture as models improve, and constantly re-evaluate your assumptions