---
layout: post
comments: false
title:  "Context Engineering in Manus"
excerpt: "Manus approaches to context engineering."
date:   2025-10-15 
---

[Lance Martin](https://x.com/RLanceMartin)

## Why Context Engineering

> Earlier this week, I had a webinar with [Manus co-founder and CSO Yichao "Peak" Ji](https://luma.com/819i5ime). You can see the video [here](https://youtu.be/6_BcCthVvb8?si=o8ovK6YNWOXtq7j7), my slides [here](https://drive.google.com/file/d/1QGJ-BrdiTGslS71sYH4OJoidsry3Ps9g/view), and Peak's slides [here](https://docs.google.com/presentation/d/1Z-TFQpSpqtRqWcY-rBpf7D3vmI0rnMhbhbfv01duUrk/edit?usp=sharing). Below are my notes.
 
[Anthropic defines](https://www.anthropic.com/engineering/building-effective-agents) agents as systems where LLMs direct their own processes and tool usage, maintaining control over how they accomplish tasks. In short, it's an LLM calling tools in a loop.

[Manus](https://en.wikipedia.org/wiki/Manus_(AI_agent)) is one of the most popular [general-purpose consumer agents](https://x.com/manusai_hq?lang=en). The typical Manus task uses [50 tool calls](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus). Without context engineering, these tool call results would accumulate in the LLM context window. As the context window fills, many have observed that LLM performance degrades. 

For example, Chroma has a great study on [context rot](https://research.trychroma.com/context-rot) and Anthropic has [explained](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents) how growing context depletes an LLM's attention budget. So, it's important to carefully manage what goes into the LLM's context window when building agents. [Karpathy laid this out clearly](https://x.com/karpathy/status/1937902205765607626?ref=blog.langchain.com):    

> Context engineering is the delicate art and science of filling the context window with just the right information for the next step (in an agent's trajectory)

## Context Engineering Approaches

Each Manus session uses a [dedicated cloud-based virtual machine](https://e2b.dev/blog/how-manus-uses-e2b-to-provide-agents-with-virtual-computers), giving the agent a virtual computer with a filesystem, terminal, various utilities, and more. 

<figure>
<img src="/assets/manus_sandbox.png" width="90%">
<figcaption>
</figcaption>
</figure>

In this virtual sandbox, Manus uses three primary strategies for context engineering, which align with approaches Anthropic covers [here](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents) and [I've seen in across many projects](https://docs.google.com/presentation/d/1Z-TFQpSpqtRqWcY-rBpf7D3vmI0rnMhbhbfv01duUrk/edit?usp=sharing):

* **Reduce**
* **Offload** 
* **Isolate**

## Context Reduction

Tool calls in Manus have a "full" and "compact" representation. The full version contains the raw content from tool invocation (e.g., a complete search tool result), which is stored in the sandbox (e.g., filesystem). The compact version stores a reference to the full result (e.g., a file path).

<figure>
<img src="/assets/manus_compaction.png" width="90%">
<figcaption>
</figcaption>
</figure>

Manus applies compaction to older ("stale") tool results as the context window approaches its limit. Newer tool results remain in full to guide the agent's next decision. This approach is similar to Anthropic's [context editing](https://www.anthropic.com/news/context-management) feature: 

> Context editing automatically clears stale tool calls and results from within the context window when approaching token limits. As your agent executes tasks and accumulates tool results, context editing removes stale content while preserving the conversation flow, effectively extending how long agents can run without manual intervention.

When compaction reaches diminishing returns (see figure below), Manus applies summarization. Summaries are generated from the trajectory using full tool results and Manus uses a schema to define the summary fields. This creates a consistent summary object for any agent trajectory. 

<figure>
<img src="/assets/manus_reduction.png" width="90%">
<figcaption>
</figcaption>
</figure>

## Context Isolation

Manus takes a pragmatic approach to multi-agent, avoiding anthropomorphized divisions of labor. While humans organize by role (designer, engineer, project manager) due to cognitive limitations, LLMs don't necessarily share these same constraints.

With this in mind, the primary goal of sub-agents in Manus is to *isolate context*. For example, if there's a discrete task to be done, Manus will assign that task to a sub-agent with its own context window.

Manus multi-agent system has a few components: a planner that assign tasks, a knowledge manager that reviews conversations and determines what should be saved in the filesystem, and an executor that performs tasks assigned by the planner. 

Manus initially used a `todo.md` for task planning, but found that roughly one-third of all actions were spent updating the todo list, wasting valuable tokens. They shifted to a  dedicated planner agent that calls executor sub-agents to perform tasks.

In a recent podcast, Erik Schluntz (multi-agent research at Anthropic) mentioned that they [similarly design multi-agent systems with a planner to assign tasks](https://youtu.be/uhJJgc-0iTQ?si=VhuFOy9uf6rDc9ya&t=688) and use function calling as the communication protocol to initiate sub-agents. A central challenge raised by Erik as well as Walden Yan (Cognition) is  [context sharing between a planner and any sub-agents](https://cognition.ai/blog/dont-build-multi-agents). 

Manus addresses this in two ways. For simple tasks (e.g., a discrete task where the planner *only needs the output* of the sub-agent), the planner simply creates instructions and passed them to the sub-agent via the function call. This resembles [Claude Code's task tool](https://claudelog.com/faqs/what-is-task-tool-in-claude-code/). 

<figure>
<img src="/assets/manus_isolation.png" width="90%">
<figcaption>
</figcaption>
</figure>

For more complex tasks (e.g., the sub-agent needs to write to files that the planner also uses), the planner shares its *full* context with the sub-agent. The sub-agent still has its own action space (tools) and instructions, but receives the *full* context that the planner also has access to. 

<figure>
<img src="/assets/manus_isolation_share.png" width="90%">
<figcaption>
</figcaption>
</figure>

In both cases, the planner defines the sub-agent's output schema. Sub-agents have a `submit results` tool to populate this schema before returning results to the planner and Manus uses constrained decoding to ensure output adhere to the defined schema. 

## Context Offloading

### Tools Definitions

We often want agents that can perform a wide range of actions. We can, of course, bind a large collection of tools to the LLM and provide detailed instructions on how to use them. One problem is that tool descriptions use valuable tokens. A deeper problem is that many (often overlapping or ambiguous) tools [can cause model confusion](https://www.anthropic.com/news/context-management).

Manus uses a layered action space with function/tool calling as well as it's virtual computer with a filesystem, terminal, various utilities. The central idea is to use a small set (< 20) of atomic functions; this includes things like a Bash tool, tools to manage the filesystem and a code execution tool. 

Rather than bloating the function calling layer, Manus *offloads* many actions to the sandbox layer. For example, it provides many utilities that can be executed via the Bash tool. As a second example, [MCP tools](https://modelcontextprotocol.io/docs/getting-started/intro) are exposed through a CLI that the agent can execute using the Bash tool.

<figure>
<img src="/assets/manus_offloading.png" width="90%">
<figcaption>
</figcaption>
</figure>

This uses an idea that I also see with Claude's new [skills](https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills) feature: progressive disclosure. Give an agent access to a computer with a set of general functions. Then, just let the agent discover actions (e.g., skills, utilities, etc) using the computer. 

> Progressive disclosure is the core design principle that makes Agent Skills flexible and scalable. Like a well-organized manual that starts with a table of contents, then specific chapters, and finally a detailed appendix, skills let Claude load information only as needed ... agents with a filesystem and code execution tools donâ€™t need to read the entirety of a skill into their context window when working on a particular task. 

### Tool Results 

In addition, Manus has access to a filesystem. As explained above, Manus *offloads* context (e.g., tool results) to the filesystem. And, like Claude Code, Manus uses basic utilities (e.g., `glob` and `grep`) to search the filesystem without the need for indexing (e.g., vectorstores). 

## Model Choice

Rather than committing to a single model, Manus uses task-level routing: it might use Claude for coding, Gemini for multi-modal tasks, or OpenAI for math and reasoning. Broadly, Manus's approach to model selection is driven by cost considerations, [with KV cache efficiency playing a central role](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus). 

Manus uses caching (e.g., for system instructions, older tool results, etc) to reduce both cost and latency across many agent turns. Peak mentioned that distributed KV cache infrastructure is challenging to implement with open source models, but is [well-supported by frontier providers](https://www.anthropic.com/news/prompt-caching). This caching support can make frontier models cheaper for certain (agent) use-cases in practice. 

## Build with the Bitter Lesson in Mind

We closed the discussion talking about [the Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html). I've been interested [in its implications for AI engineering](https://rlancemartin.github.io/2025/07/30/bitter_lesson/). Boris Cherny (creator of Claude Code) mentioned that [The Bitter Lesson](https://www.youtube.com/watch?v=Lue8K2jqfKk) influenced his decision to keep Claude Code unopinionated, making it easier to adapt to model improvements.

Building on constantly improving models means accepting constant change. Peak mentioned that Manus has been refactored five times since their launch in March! 

In addition, Peak warned that the agent's harness can limit performance as models advance; this is exactly the challenge called out by the Bitter Lesson. We add structure to improve performance at a point in time, but this structure can limit performance as compute (models) grows.

To guard against this, Peak suggested running agent evaluations across varying model strengths. If performance doesn't improve with stronger models, your harness may be hobbling the agent. This can help test whether your harness is "future proof".

Hyung Won Chung's (OpenAI/MSL) [talk](https://youtu.be/orDKvo8h71o?si=fsZesZuP25BU6SqZ) on this topic further emphasizes the need to consistently re-evaluate structure (e.g., your harness / assumptions) as models improve.

> *Add structures needed for the given level of compute and data available. 
Remove them later, because these shortcuts will bottleneck further improvement.*
> 

<figure>
<img src="/assets/bitter_lesson_timeline.png" width="90%">
<figcaption>
</figcaption>
</figure>

## Conclusions

Key take-aways from the discussion:

**Context Reduction:**
1. **Compact stale results** - Replace older tool results with references (e.g., file paths) as context fills, keeping recent results in full
2. **Summarize strategically** - Once compaction reaches diminishing returns, apply schema-based summarization to the full trajectory for consistency

**Context Offloading:**
3. **Offload tool results** - Store full tool results in the filesystem (external to context) and access on demand using basic utilities like `glob` and `grep`
4. **Use progressive disclosure** - Give agents a small set of general tools (Bash, filesystem, code execution) and push other utilities to the sandbox layer rather than overloading the function calling layer

**Context Isolation:**
5. **Isolate context, not roles** - Use sub-agents primarily to isolate context for discrete tasks, not to anthropomorphize roles (designer, engineer, etc)
6. **Share context deliberately** - For simple tasks, pass only instructions; for complex tasks where sub-agents need shared state, pass the full context
7. **Define output schemas** - Always specify schemas for sub-agent results and use constrained decoding to ensure compliance

**System Design:**
8. **Layer your action space** - Use few atomic function calls that access a rich sandbox environment (filesystem, terminal, utilities, MCP CLI)
9. **Cache for efficiency** - Aggressive KV caching (system instructions, tool results) makes frontier models cost-competitive with open source

**Development Philosophy:**
10. **Test across model strengths** - If stronger models don't improve performance, your harness may be limiting them (a "future-proof" test)
11. **Embrace continuous refactoring** - Accept that constantly improving models require architecture changes (Manus refactored 5x since March launch)
12. **Stay unopinionated** - Simple, flexible designs adapt better to model improvements than rigid, heavily-structured systems (The Bitter Lesson)