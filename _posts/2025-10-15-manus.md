
# Context Engineering with Manus

## Why Context Engineering

> Preface: Earlier this week, I had a webinar with [Manus co-founder and CSO Yichao "Peak" Ji](https://luma.com/819i5ime). You can see the video [here](https://youtu.be/6_BcCthVvb8?si=o8ovK6YNWOXtq7j7). You can see my slides [here](https://drive.google.com/file/d/1QGJ-BrdiTGslS71sYH4OJoidsry3Ps9g/view) and Peak's slides [here](https://docs.google.com/presentation/d/1Z-TFQpSpqtRqWcY-rBpf7D3vmI0rnMhbhbfv01duUrk/edit?usp=sharing). Below are some of my notes from the talk. 
 
[Anthropic defines LLM powered autonomous agents](https://www.anthropic.com/engineering/building-effective-agents) as systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks. In short, they are an LLM calling tools autonomously in a loop.

[Manus](https://en.wikipedia.org/wiki/Manus_(AI_agent)) is one of the most popular [general-purpose agent products](https://x.com/manusai_hq?lang=en). The typical Manus task uses [50 tool calls](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus). Without context engineering, these tool call results would accumulate in the LLM context window. As the context window fills, many have observed that LLM performance degrades. 

For example, Chroma has a great study on [context rot](https://research.trychroma.com/context-rot) and Anthropic has [explained](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents) how growing context depletes an LLM's attention budget. So, it's important to carefully manage what goes into the agent's context window. [Karpathy stated this clearly](https://x.com/karpathy/status/1937902205765607626?ref=blog.langchain.com):    

> Context engineering is the delicate art and science of filling the context window with just the right information for the next step (in an agent's trajectory)

## Context Engineering Approaches

Each Manus session runs in full virtual machine, which includes a filesystem and various utilities explained below. Within this environment, Manus uses three primary strategies for context engineering. These align with approaches [I've seen in across many other projects](https://docs.google.com/presentation/d/1Z-TFQpSpqtRqWcY-rBpf7D3vmI0rnMhbhbfv01duUrk/edit?usp=sharing) and with the 3 categories mentioned by Anthropic [here](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents). The three categories are:

* **Reduce**: Minimize the size of information added to the context window through smart tool design, compaction of tool outputs, and summarization of conversation history.
* **Offload**: Keep raw data and tool observations external (e.g., in the filesystem or memory of a sandbox environment) to the agent's context window.
* **Isolate**: Delegate specific tasks to specialized sub-agents that operate with their own isolated context windows, preventing context bloat in the main agent.

## Reduce: Compaction and Summarization

Many tool calls in Manus have a "full" and "compact" representation. The full version contains the full content from tool invocation (e.g., a complete search tool result), which lives in the sandbox (e.g., filesystem). The compact version stores a reference to the full result (e.g., a file path or url).

Manus uses a policy to apply compaction as the agent approaches the context window limit based on recency (e.g., the oldest % of tool result messages will be repleaced with the compact version). Newer tool calls remain in full, ensuring the model has complete recent context for its next decision. This approach is similar to Anthropic's recent [context editing](https://www.anthropic.com/news/context-management) feature.

When compaction reaches diminishing returns, Manus then applies summarization. Critically, summaries are always generated from the full tool results. Rather than using free-form prompting, Manus uses a schema to define specific fields for extraction during summarization. This creates stable, structured summary outputs that are consistent across any agent message history. 

## Isolate: Strategic Sub-Agent Design

Manus takes a pragmatic approach to sub-agents, avoiding the anthropomorphized divisions of labor common in some multi-agent systems. While humans organize by role (designer, engineer, project manager) due to cognitive limitations, Manus recognizes that LLMs don't share these same constraints.

The system uses just a few specialized sub-agents: a planner that manages work, a knowledge manager that reviews conversations and determines what should be saved in the filesystem, and an executor for delegating tasks. This minimalism stems from the fact that [inter-agent communication is difficult](https://cognition.ai/blog/dont-build-multi-agents) and every sub-agent adds communication overhead.

Manus initially used a `todo.md` for task planning, but found that roughly one-third of all actions were spent updating the todo list, wasting valuable tokens! They shifted to an agent-as-tool approach where a sub-agent (executor) can be invoked as a tool call for specific tasks.

Syncing context between the planner agent and the executor sub-agent presents challenges that Manus addresses through two approaches. For tasks with short instructions where the planner only needs the output, they share instructions: the parent sends custom instructions to the sub-agent, whose context contains only these instructions and relevant tools. This pattern resembles [Claude Code's task tool](https://claudelog.com/faqs/what-is-task-tool-in-claude-code/). 

For more complex scenarios, the parent shares its *full* context with the sub-agent. The sub-agent still has its own action space (tools) and prompt but receives a large pre-filled input containing the full context window of the parent (e.g., the full conversation history). 

Across all cases, the planner defines the sub-agent's output schema. Sub-agents have a "submit results" tool to return results to the planner. Manus uses constrained decoding to ensure outputs adhere to the defined schema. 

## Offload: A Layered Action Space

When building agents, it is tempting to provide them with a large collection of tools. In practice, tool descriptions utilize valuate tokens in agent's instructions and an excessive number of (overlapping) tools [can cause model confusion](https://www.anthropic.com/news/context-management).

One approach is to [index tool descriptions](https://github.com/langchain-ai/langgraph-bigtool) and retrieve specific tools on demand. However, this [can confuse the model](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus) if retrieved tools differ from tools called a prior turns in the chat history.

Manus employs a layered action space with three tiers: function calling, sandbox utilities, and packages/APIs. The function calling layer is *minimal*, containing fewer than 10-20 atomic functions. With simple, but general functions - such as a shell tool (that executes arbitrary shell commands) and a text editor (that read and write files) - an agent can perform an extremely wide range of tasks. 

These tasks are executed in the sandbox layer, which is an approach featured in the popular [CodeACT](https://arxiv.org/abs/2402.01030) paper. This approach is elegant, because many utilities can simply be exposed as shell commands and executed in the sandbox. Many pre-installed utilities—speech tools, MCP CLI, and others—are available as shell commands. The system prompt informs Manus about these utilities and instructs it to use `--help` flags to learn about them.

This design allows Manus to add new capabilities without touching the model's function calling space. MCP tools, for instance, aren't injected into function calls but are added to the CLI layer. The agent discovers them organically, keeping the core action space stable while enabling extensibility.

In addition, Manus has access to a filesystem. Like Claude Code, Manus uses basic UNIX utilities like glob and grep to search the filesystem and does not rely on indexing or vector stores.

As an example, for a task that required analysis the planner agent will spawn an executor sub-agent to perform the analysis. The executor sub-agent makes a tool call to write a script that implements the analysis logic (e.g., calling any number of API available in the sandbox). It then makes a tool call to execute the script and write the results to a log. It can then read the results and only *then* return a summary of the results to the planner agent's context window.

## Model Choice: Pragmatism Over Purity

Manus's approach to model selection is driven by cost considerations, [with KV cache efficiency playing a central role](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus). They use caching aggressively—for system instructions and other repeated content—to reduce both cost and latency. Distributed KV cache infrastructure is challenging to implement with open source models but is [well-supported by frontier providers](https://www.anthropic.com/news/prompt-caching), sometimes making frontier models cheaper (than open source models) in practice.

Rather than committing to a single model, Manus mixes and matches with task-level routing: for example, they might use Claude for coding, Gemini for multi-modal tasks, and OpenAI for math and reasoning. This pragmatism extends to their stance on reinforcement learning and fine-tuning.

While fine-tuning is tempting, Manus advises against building specialized models before achieving product-market fit. Reinforcement learning works well with a fixed action space, but fixing the action space is difficult in the current agent landscape.

The launch of MCP exemplifies this challenge. Supporting MCP transformed Manus from having a static action space to requiring an extensible one, creating a much harder open-domain problem for RL. They found it better to lean on general foundation models and context engineering. The most interesting future areas of work Peak mentioned are parameter-free online learning for personalized memory. 

## Build with the Bitter Lesson in Mind

I've been very interested in [AI engineering with the Bitter Lesson in mind](https://rlancemartin.github.io/2025/07/30/bitter_lesson/). Boris of Claude Code mentions that [The Bitter Lesson](https://www.youtube.com/watch?v=Lue8K2jqfKk) strongly included his approach; keeping Claude Code simple and unopionated makes it easier to adapt to model improvements.

Building on constantly improving models means accepting constant change. Peak mentioned that Manus has refactored five times since their launch in March. 

Your harness can limit performance as models advance. To guard against this, run evaluations across varying model strengths with a fixed harness. If performance doesn't improve with stronger models, your harness may be hobbling the agent. The bitter lesson applies to agent systems too: general methods that scale with computation and better models ultimately prevail over specialized approaches.