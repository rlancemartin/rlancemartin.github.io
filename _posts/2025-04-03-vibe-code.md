---
layout: post
comments: false
title:  "Vibe code benchmark"
excerpt: "Comparing code agents and ways of connecting them with data."
date:   2025-04-03 
---

[@rlancemartin](https://x.com/RLanceMartin)

### Summary

I compared Cursor and Claude Code on a set of "vibe code" challenges, giving them access to external documentation in 4 different ways. A well-crafted `llms.txt` file paired with Claude Code outperformed all other approaches.

<figure>
<img src="/assets/vibe_code_benchmark.png" width="90%">
<figcaption>
</figcaption>
</figure>

### Challenge

Code agents like [Cursor](https://www.cursor.com/) and [Claude Code](https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview) are transforming software development. Protocols like [MCP (Model Context Protocol)](https://www.anthropic.com/news/model-context-protocol) connect these agents with external data sources. I was curious about the performance of different code agents and the various ways to connect them with data.

I selected 5 code questions (see prompts [here](https://github.com/rlancemartin/vibe-code-benchmark?tab=readme-ov-file#langgraph-challenges)) that require [LangGraph](https://langchain-ai.github.io/langgraph/) documentation (~260k tokens). For each question, I prompted Claude Code and Cursor and allowed them to complete the task autonomously (by accepting all tool calls / diffs aka ["vibe coding"](https://x.com/karpathy/status/1886192184808149383?lang=en)). I gave them access to LangGraph documentation in different ways:

1. **Context Stuffing**: I gave all [LangGraph docs](https://langchain-ai.github.io/langgraph/llms-full.txt) (~260k tokens) to the code agent.

2. **Standard llms.txt**: [`llms.txt`](https://llmstxt.org/) files provide background information, links, and page descriptions to LLMs. I gave the code agent an [MCP server](https://github.com/langchain-ai/mcpdoc) with access to the [LangGraph `llms.txt` file](https://github.com/langchain-ai/langgraph/pull/3987/commits/43e039d97faa7d0b4af2e407c64d830379f02ab4) and a URL loader tool.

3. **Optimized llms.txt**: I used an [LLM to re-write](https://github.com/rlancemartin/llmstxt_architect) the LangGraph `llms.txt` file with clearer, more consistent URL descriptions and gave this to the code agent via an MCP server.

4. **Vector Database**: I built a [vector database](https://github.com/langchain-ai/vibe-code-benchmark/blob/main/context_and_mcp/build_langgraph_context.py) of the LangGraph documentation (8k token chunks, k=3, OpenAI `text-embedding-3-large`) and gave it to the code agent via an [MCP server](https://github.com/langchain-ai/vibe-code-benchmark/blob/main/context_and_mcp/langgraph_vectorstore_mcp.py) for semantic search.

<figure>
<img src="/assets/llm_context.png" width="90%">
<figcaption>
</figcaption>
</figure>

### Evaluation

Each of the 5 code question prompts instructed the agent to generate a script, which I evaluated:

1. **Import Success** (0-1 points): Can the code be imported without errors?
2. **Run Success** (0-1 points): Does the script run without error?
3. **LLM Quality Score** (0-1 points): Does the output meets the challenge requirements?
4. **Deployment Score** (0-0.5 points): Can the code be [deployed](https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/)?

Each script could earn up to 3.5 points, with a maximum of 17.5 points possible per experiment (5 challenges Ã— 3.5 points). I ran each experiment on separate branches to prevent cross-contamination, with all generated code stored in the [experiments folders here](https://github.com/rlancemartin/vibe-code-benchmark/tree/main/expts), evaluation code [here](https://github.com/rlancemartin/vibe-code-benchmark/tree/main/eval), results [here](https://github.com/rlancemartin/vibe-code-benchmark/tree/main/eval/logs/eval_run_20250402).

### Results

<figure>
<img src="/assets/vibe_code_benchmark.png" width="90%">
<figcaption>
</figcaption>
</figure>

For Claude Code and Cursor, I see same pattern: optimized `llms.txt` > vector database > standard `llms.txt` > context stuffing. You can think about `llms.txt` as RAG with full documents as retrieval units. The LLM just reviews the URL descriptions in `llms.txt` and decides which documents to retrieve (via a [URL loader tool call](https://github.com/langchain-ai/mcpdoc)). 

It's a simple approach, but relies on [well written document descriptions](https://github.com/rlancemartin/llmstxt_architect): I noticed that the descriptions were pretty bad in the initial `llms.txt` file. So, I had an LLM read every URL and summarize it ([here's](https://github.com/rlancemartin/llmstxt_architect) the little package I made for this), creating the optimized `llms.txt`. This performed a lot better on the benchmark.

Vector databases scale well, but retrieval can be sensitive to things like embedding, chunk size, k-nearest neighbors, etc. This makes them a bit tricky to get right, and it's possible I didn't tune it as well as I could have (e.g., maybe my chunk size or k were too small, and failed to retrieve the correct or sufficient documents).

Context stuffing is the simplest approach, but appears to be less effective when working with the code agents; the fairly large context (266k tokens) of the documentation here presents surface area for the LLM to conflate concepts from different parts of the documentation. 

You can see all errors for each approach in the log [here](https://github.com/rlancemartin/vibe-code-benchmark/blob/main/eval/logs/eval_run_20250402/eval_report_20250402_221445.txt): Import/Module Errors are the most common, suggesting that code agents have a tendency to hallucinate APIs that *seem* plausible but don't exist (as shown below).

```shell
from langgraph.prebuilt.tool_executor import ToolExecutor
ModuleNotFoundError: No module named 'langgraph.prebuilt.tool_executor'
```

The strongest result was from Claude Code with the optimized `llms.txt` file. This was able to solve the hardest challenge, which was correctly formatting the `langgraph.json` file as scripts were added so that they can be [deployed](https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/) locally. This required understanding the [LangGraph server application structure](https://langchain-ai.github.io/langgraph/concepts/application_structure/) from one documentation page while understanding other documents related to each specific challenge. 

## Conclusion

How we connect context to code agents impacts their performance. The optimized `llms.txt` approach with Claude Code achieved the best overall results and offers a nice approach that's fairly easy to understand and implement. It's easy to set this up with an [MCP server](https://github.com/langchain-ai/mcpdoc) and I liked the ability to audit the tool calls and resulting tool messages (fetched docs) with the code agents. Still, it's not clear how well this will scale to large numbers of URLs and some effort is required to write good document descriptions.

### Notes and Acknowledgements

Thanks to [@veryboldbagel](https://x.com/veryboldbagel) for the helpful feedback, discussion, and work on the MCP server / llms.txt files. The experiments ran with Claude Code (`claude-3-7-sonnet-20250219`) and Cursor version 0.47.8(`claude-3-7-sonnet` in thinking mode).
